{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fdsig/iaqa/blob/main/visualize_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XV4ml-NAaBUT"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qe657S4PVlyZ"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "# get dependacies \n",
        "!pip -q install vit_pytorch linformer\n",
        "!rm -rf vit-pytorch/ && rm -rf image_utils\n",
        "# to obtian AVA Dataset withough needing to dowload or mount your own drive\n",
        "!git clone https://github.com/fdsig/image_utils\n",
        "# moves image getter (ava data) into working directory\n",
        "!mv image_utils/image_getter.py ./image_getter.py\n",
        "!pip install timm\n",
        "!pip uninstall albumentations -y\n",
        "!pip install albumentations\n",
        "!pip uninstall opencv-python-headless==4.5.5.62 -y\n",
        "!pip install opencv-python-headless==4.5.2.52  \n",
        "!pip install wandb -qqq\n",
        "!pip install git+https://github.com/rwightman/pytorch-image-models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "--JChSsPY03P",
        "outputId": "ee80dfb1-b278-4777-c5b5-9292eb47dd44"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# wandb integration loging in at start\n",
        "import wandb\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_P2u9wyv1Q8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7acb6c-481e-4e3e-b9a4-cbbdd682b011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "def mount():\n",
        "    #mount drive for saving results \n",
        "    #need to go through the usual authorization dance\n",
        "    drive.flush_and_unmount()\n",
        "    drive.mount('/content/drive', force_remount=True) \n",
        "mount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyNdFo4Pwo-W",
        "outputId": "83cad7a8-c9b0-4347-c5e6-233f6c25b772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch_0.zip   Batch_18.zip  Batch_26.zip  Batch_34.zip\tBatch_42.zip\n",
            "Batch_10.zip  Batch_19.zip  Batch_27.zip  Batch_35.zip\tBatch_43.zip\n",
            "Batch_11.zip  Batch_1.zip   Batch_28.zip  Batch_36.zip\tBatch_4.zip\n",
            "Batch_12.zip  Batch_20.zip  Batch_29.zip  Batch_37.zip\tBatch_5.zip\n",
            "Batch_13.zip  Batch_21.zip  Batch_2.zip   Batch_38.zip\tBatch_6.zip\n",
            "Batch_14.zip  Batch_22.zip  Batch_30.zip  Batch_39.zip\tBatch_7.zip\n",
            "Batch_15.zip  Batch_23.zip  Batch_31.zip  Batch_3.zip\tBatch_8.zip\n",
            "Batch_16.zip  Batch_24.zip  Batch_32.zip  Batch_40.zip\tBatch_9.zip\n",
            "Batch_17.zip  Batch_25.zip  Batch_33.zip  Batch_41.zip\n"
          ]
        }
      ],
      "source": [
        "!ls drive/MyDrive/ava_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uALzNNdNVs7u"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import json\n",
        "import cv2\n",
        "import glob\n",
        "from itertools import chain\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "# file handling\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from linformer import Linformer\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from vit_pytorch.efficient import ViT\n",
        "from sklearn.utils import class_weight\n",
        "import time\n",
        "import copy\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#\n",
        "import timm\n",
        "from timm.scheduler import create_scheduler\n",
        "from timm.optim import create_optimizer\n",
        "\n",
        "from albumentations import pytorch\n",
        "from pathlib import Path\n",
        "import torchvision\n",
        "import gc\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn import metrics\n",
        "from vit_pytorch.cvt import CvT\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z4HjHQqgXHub"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C1Pn0ublvnIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06656e2f-f81f-4a6a-e22b-4cac449d61ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[38;2;255;105;180m██████████\u001b[0m| 44/44 [10:41<00:00, 14.58s/it]\n"
          ]
        }
      ],
      "source": [
        "# equivalent to:\n",
        "#!cp -r drive/MyDrive/ava_batches_demo batches\n",
        "origionals = Path('drive/MyDrive/ava_batches/')\n",
        "target = Path('batches/')\n",
        "if not target.exists():\n",
        "  target.mkdir()\n",
        "fids = [fid for fid in origionals.iterdir()]\n",
        "for fid in tqdm(fids,colour=(\"#FF69B4\")):\n",
        "  shutil.copy(fid.as_posix(),target/fid.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9BXBMOdbN9f",
        "outputId": "b283f0a9-56cf-479c-d0bb-17f699bdec93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[38;2;255;105;180m██████████\u001b[0m| 44/44 [04:36<00:00,  6.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55020.jpg\n"
          ]
        }
      ],
      "source": [
        "paths = [i for i in os.scandir('batches')]\n",
        "for zip_f in tqdm(paths, colour=(\"#FF69B4\")):\n",
        "                zip_ref = zipfile.ZipFile(zip_f, \"r\")\n",
        "                zip_ref.extractall(\"./Images/\")\n",
        "                zip_ref.close()\n",
        "                os.remove(zip_f)\n",
        "\n",
        "      \n",
        "\n",
        "root = Path(\"Images\")\n",
        "images = [file for folder in root.iterdir() for file in folder.iterdir()]\n",
        "print(images[0].name)\n",
        "for img in images:\n",
        "  img.rename(root/img.name)\n",
        "\n",
        "for i in root.iterdir():\n",
        "  if i.is_dir():\n",
        "    i.rmdir()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import rcParams\n",
        "def scalar_resize(fid,scalar=None):\n",
        "    img = cv2.imread(fid.path, cv2.IMREAD_UNCHANGED)\n",
        "    shape = np.array(img.shape)\n",
        "    scalar = scalar/shape[shape.argmax()]\n",
        "    shape = np.ceil(shape*scalar).astype(int)\n",
        "    dim = (shape[1], shape[0])\n",
        "    # resize image\n",
        "    return cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "def get_df():\n",
        "    df = pd.read_csv('/content/image_utils/ava_meta_with_int_id_230721.csv')\n",
        "    display(df)\n",
        "    plt.hist(df['MLS'].values.ravel(), bins=10)\n",
        "    plt.show()\n",
        "    return df\n",
        "\n",
        "def meta_process(df=None):\n",
        "    y_gt = df['mos_float'].values\n",
        "    ids = df['ID'].values\n",
        "    print(len(ids))\n",
        "    y_gt_std, y_gt_mean = np.std(y_gt, axis = 0), np.mean(y_gt, axis = 0)\n",
        "    exclude_below = y_gt_mean-y_gt_std*4\n",
        "    exclude_above = y_gt_mean+y_gt_std*4\n",
        "    ids = ids[np.argwhere(y_gt>=exclude_below)].ravel()\n",
        "    y_gt = y_gt[np.argwhere(y_gt>=exclude_below)].ravel()\n",
        "    print(len(y_gt))\n",
        "    ids = ids[np.argwhere(y_gt<=exclude_above)].ravel()\n",
        "    y_gt = y_gt[np.argwhere(y_gt<=exclude_above)].ravel()\n",
        "    print(len(ids),len(y_gt))\n",
        "    ids_low = ids[np.argwhere(y_gt<5)].ravel().astype(int)\n",
        "    ids_high = ids[np.argwhere(y_gt>5)].ravel().astype(int)\n",
        "    to_include = np.concatenate((ids_low,ids_high), axis=0)\n",
        "    len(to_include)\n",
        "    return df[df['ID'].isin(to_include)]\n",
        "\n",
        "\n",
        "def one_hot(df):\n",
        "    one_hot = pd.get_dummies(df['MLS'])\n",
        "    one_hot = pd.merge(df['ID'],one_hot, right_on=df.index, left_index=True)\n",
        "    one_hot = one_hot[one_hot.columns[1:]]\n",
        "    y_df = pd.merge(one_hot,df[['threshold', 'ID','MOS','MLS','set']], right_on=one_hot.index, left_index=True)\n",
        "    return y_df[y_df.columns[2:]]\n",
        "\n",
        "\n",
        "\n",
        "def sort_show():\n",
        "    ava = [ i.path for i in os.scandir('Images/')]\n",
        "    ava.sort()\n",
        "    read = lambda fid: cv2.cvtColor(cv2.imread(fid), \n",
        "                                    cv2.COLOR_BGR2RGB)\n",
        "    img = read(ava[0])\n",
        "    print(img.shape)\n",
        "    fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
        "    ax.imshow(img)\n",
        "\n",
        "\n",
        "def get_labels(df):\n",
        "    y_df = one_hot(df)\n",
        "    labels = (\n",
        "            fid.name.split('.')[0] \n",
        "            for path in os.scandir('Images/') \n",
        "            for fid in os.scandir(path.path))\n",
        "    y_g = y_df.to_dict('index')\n",
        "    return {str(y_g[pair_key]['ID_y']):y_g[pair_key] for pair_key in y_g}\n",
        "\n",
        "def make_class_dir(df,y_g_dict):\n",
        "    '''creates text train val with class subdirs\n",
        "    \n",
        "    ⌊_train\n",
        "    |     ⌊_class 0\n",
        "    |     ⌊_class 1\n",
        "    ⌊_test\n",
        "    |     ⌊_class 0\n",
        "    |     ⌊_class 1\n",
        "    ⌊_val_\n",
        "          ⌊_class 0\n",
        "          ⌊_class 1'''\n",
        "    \n",
        "    os.makedirs('data/', exist_ok=True)\n",
        "    train_dir = 'data/train/'\n",
        "    test_dir = 'data/test/'\n",
        "    #!rm -rf data/train/ && rm -rf data/test/\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir,exist_ok=True)\n",
        "    not_loaded_train, not_loaded = [ ], [ ]\n",
        "    test_df = df[df['set']=='test']\n",
        "    files_ = [i.name for i in os.scandir('Images/')]\n",
        "    test_set = test_df['image_name'].values\n",
        "    for im_id in tqdm(test_set, colour=('#FF69B4')):\n",
        "        key = im_id.strip('.jpg')\n",
        "        y_g_dict[key]['fid']='data/test/'+im_id\n",
        "        try:\n",
        "        \n",
        "            os.symlink('Images/'+im_id,'data/test/'+im_id)\n",
        "        except:\n",
        "            not_loaded.append(im_id)\n",
        "    train_df = df[df['set'].isin(['training','validation'])]\n",
        "    train_set = train_df['image_name'].values\n",
        "    for im_id in tqdm(train_set, colour=('#FF69B4')):\n",
        "        key = im_id.strip('.jpg')\n",
        "        y_g_dict[key]['fid']='data/train/'+im_id\n",
        "        try:\n",
        "        \n",
        "            os.symlink('Images/'+im_id,'data/train/'+im_id)\n",
        "        except:\n",
        "            not_loaded_train.append(im_id)\n",
        "            \n",
        "    return y_g_dict\n",
        "    \n",
        "\n",
        "def class_wts(df):\n",
        "    '''computes class weights for training samplere'''\n",
        "    y_gt = df.values.ravel()\n",
        "    y_gt_ = np.array(y_gt)\n",
        "    y = np.bincount(y_gt_)\n",
        "    x = np.unique(y_gt_)\n",
        "    print(len(y),len(x))\n",
        "    plt.bar(x,y)\n",
        "    plt.show()\n",
        "    class_weights=class_weight.compute_class_weight('balanced',classes=x,y=y_gt_)\n",
        "    class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
        "    print(class_weights) \n",
        "    return class_weights, y\n",
        "\n",
        "def image_plot(image_dict, eval_list=None, super_title = None, n_images=None, evaluate=None):\n",
        "    '''plots random images'''\n",
        "    rcParams['axes.titlepad'] = 10\n",
        "    if not evaluate:\n",
        "        random_keys = np.random.choice(\n",
        "        list(image_dict.keys()), n_images, replace=False\n",
        "        )\n",
        "    if evaluate:\n",
        "        eval_list = [\n",
        "                     name.split('.')[0] for name in eval_list\n",
        "                     ]\n",
        "        random_keys = np.random.choice(\n",
        "        eval_list, n_images, replace=False\n",
        "        )\n",
        "        \n",
        "\n",
        "    cvt = lambda img: cv2.cvtColor(cv2.imread(img, cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
        "    if random_keys.__class__==np.ndarray:\n",
        "        fig, axes = plt.subplots(1,len(random_keys), figsize=(15,10))\n",
        "        for idx, ax in enumerate(axes.ravel()):\n",
        "            print(random_keys)\n",
        "            img = cvt(os.readlink(image_dict[random_keys[idx]]['fid']))\n",
        "            ax.axis(\"off\")\n",
        "            ax.set_yticks([])\n",
        "            ax.set_xticks([])\n",
        "            meta = image_dict[random_keys[idx]]\n",
        "            title = f\"ID:{random_keys[idx]} MOS:{meta['MOS']:.2f}\"\n",
        "            \n",
        "            if meta['threshold'] ==1:\n",
        "                cls_ = 0\n",
        "            else:\n",
        "                cls_ = 1\n",
        "            title += f\"\\nBinary Class: {cls_}\"\n",
        "            ax.set_title(title, size=20)\n",
        "            ax.imshow(img)\n",
        "        fig.suptitle(super_title, fontsize=24)\n",
        "        fig.savefig(super_title+'.png')\n",
        "        plt.show()\n",
        "    else:\n",
        "    \n",
        "        fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
        "        ax.axis(\"off\")\n",
        "        print(random_keys)\n",
        "        img = cvt(os.readlink(image_dict[random_keys]['fid'])) \n",
        "        ax.set_yticks([])\n",
        "        ax.set_xticks([])\n",
        "        meta = image_dict[random_keys]\n",
        "        title = f\"ID:{random_keys} MOS:{meta['MOS']:.2f}\"\n",
        "        if meta['threshold'] ==1:\n",
        "            cls_ = 0\n",
        "        else:\n",
        "            cls_ = 1\n",
        "        title += f\"\\nBinary Class: {cls_}\"\n",
        "        title += f\"\\nBinary Class: {meta['threshold']}\"\n",
        "        ax.set_title(title, size=20)\n",
        "        ax.imshow(img)\n",
        "        fig.suptitle(super_title, fontsize=24)\n",
        "        fig.savefig(super_title+'.png')\n",
        "        plt.show()\n",
        "\n",
        "def get_all():\n",
        "    '''meta fucntion for calling other fuctions'''\n",
        "    df = get_df()\n",
        "    df = meta_process(df=df)\n",
        "    class_weights, class_counts = class_wts(df['threshold'])\n",
        "    y_g_dict = get_labels(df)\n",
        "    make_class_dir(df,y_g_dict)\n",
        "    y_g_neg = {key:y_g_dict[key] for key in y_g_dict if y_g_dict[key]['threshold']==0}\n",
        "    y_g_pos = {key:y_g_dict[key] for key in y_g_dict if y_g_dict[key]['threshold']==1}\n",
        "    sets = ['test', 'training', 'validation']\n",
        "    splits ={\n",
        "        set_: {\n",
        "            im_key:y_g_dict[im_key] for im_key in y_g_dict \n",
        "            if y_g_dict[im_key]['set']==set_\n",
        "            } for set_ in sets\n",
        "            }\n",
        "    print(f\"train set n = {len(splits['training'])} \\ntest_list n = {len(splits['test'])}\\nvalidation_list n = {len(splits['validation'])}\")\n",
        "    return df, y_g_dict, splits, y_g_neg, y_g_pos\n",
        "\n",
        "def data_transforms(size=None):\n",
        "    '''defines data transform and returns a dict with test,train,val transforms'''\n",
        "    mask1 = np.full(30 * 140, False) \n",
        "    a_train_transform = A.Compose(\n",
        "        [\n",
        "         A.augmentations.transforms.GridDistortion(\n",
        "                num_steps=5, \n",
        "                distort_limit=0.6, \n",
        "                interpolation=1, \n",
        "                border_mode=4, \n",
        "                value=4, \n",
        "                mask_value=2, \n",
        "                always_apply=False, \n",
        "                p=0.1),\n",
        "            A.augmentations.geometric.resize.LongestMaxSize(\n",
        "                max_size=size\n",
        "                ),\n",
        "            A.augmentations.transforms.PadIfNeeded(size,size),\n",
        "            A.augmentations.transforms.Normalize(\n",
        "                mean=(0.485, 0.456, 0.406), \n",
        "                std=(0.229, 0.224, 0.225), \n",
        "                max_pixel_value=255.0, p=1.0),\n",
        "            #A.augmentations.transforms.MedianBlur(blur_limit=7, always_apply=False, p=0.5)\n",
        "            A.augmentations.transforms.CoarseDropout (max_holes=10, \n",
        "                                                      max_height=72, \n",
        "                                                      max_width=72, \n",
        "                                                    min_holes=3, \n",
        "                                                    min_height=36, \n",
        "                                                    min_width=36, \n",
        "                                                    fill_value=(random.uniform(0, 1),\n",
        "                                                                random.uniform(0, 1), \n",
        "                                                                random.uniform(0, 1)) , \n",
        "                                                      mask_fill_value=(0.5,0.2,0.4), \n",
        "                                                      always_apply=False, p=0.1),\n",
        "            A.augmentations.transforms.ColorJitter(brightness=0.05, \n",
        "                                                   contrast=0.05, \n",
        "                                                   saturation=0.05, \n",
        "                                                   hue=0.05, \n",
        "                                                   always_apply=False,\n",
        "                                                   p=0.1),\n",
        "            A.augmentations.transforms.Cutout(\n",
        "                num_holes=8, \n",
        "                max_h_size=36, \n",
        "                max_w_size=36, \n",
        "                fill_value=(\n",
        "                    random.uniform(0, 1),\n",
        "                    random.uniform(0, 1), \n",
        "                    random.uniform(0, 1)\n",
        "                    ),\n",
        "                always_apply=False, \n",
        "                p=0.1\n",
        "                ),\n",
        "            A.augmentations.transforms.GaussianBlur(\n",
        "                blur_limit=(3, 5), \n",
        "                sigma_limit=0, \n",
        "                always_apply=False, \n",
        "                p=0.1\n",
        "                ),\n",
        "            A.augmentations.transforms.GaussNoise(\n",
        "                var_limit=(0.1, 0.1), \n",
        "                mean=0.1, \n",
        "                per_channel=True, \n",
        "                always_apply=False, \n",
        "                p=0.1\n",
        "                ),\n",
        "            A.augmentations.transforms.HueSaturationValue(\n",
        "                hue_shift_limit=0.1, \n",
        "                sat_shift_limit=0.1, \n",
        "                val_shift_limit=0.1, \n",
        "                always_apply=False, \n",
        "                p=0.01\n",
        "                ),\n",
        "            A.augmentations.transforms.MotionBlur(\n",
        "                blur_limit=3,p=0.2\n",
        "                ),\n",
        "            A.augmentations.geometric.rotate.SafeRotate(limit=30, \n",
        "                                                        interpolation=1, \n",
        "                                                        border_mode=4, \n",
        "                                                        value=None, \n",
        "                                                        mask_value=None, \n",
        "                                                        always_apply=False, \n",
        "                                                        p=0.1\n",
        "                                                        ),\n",
        "        \n",
        "            A.pytorch.transforms.ToTensorV2(\n",
        "                transpose_mask=False, p=1.0\n",
        "                )\n",
        "            ]\n",
        "            )\n",
        "\n",
        "    a_test_transform = A.Compose([\n",
        "            A.augmentations.geometric.resize.LongestMaxSize(max_size=224),\n",
        "            A.augmentations.transforms.PadIfNeeded(224,224),\n",
        "            A.augmentations.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), \n",
        "                                                max_pixel_value=255.0, p=1.0),\n",
        "                            A.pytorch.transforms.ToTensorV2(transpose_mask=False, p=1.0) ]\n",
        "                            )\n",
        "\n",
        "\n",
        "\n",
        "    a_valid_transform = A.Compose([\n",
        "            A.augmentations.geometric.resize.LongestMaxSize(max_size=224),\n",
        "            A.augmentations.transforms.PadIfNeeded(224,224),\n",
        "            A.augmentations.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), \n",
        "                                                max_pixel_value=255.0, p=1.0),\n",
        "                            A.pytorch.transforms.ToTensorV2(transpose_mask=False, p=1.0) ]\n",
        "                            )\n",
        "    return {'test':a_test_transform, 'training':a_train_transform , 'validation':a_valid_transform}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_transform(data_dict):\n",
        "    '''plots data transforms'''\n",
        "    idx = 11\n",
        "    read = lambda fid: cv2.cvtColor(cv2.imread(os.readlink(fid)), cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "\n",
        "\n",
        "    cvt = lambda img: cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_fid = y_g_dict[list(y_g_dict.keys())[idx]]['fid']\n",
        "    img = read(img_fid).astype(np.uint8)\n",
        "    img_array = read(y_g_dict[list(y_g_dict.keys())[idx]]['fid'])\n",
        "\n",
        "    #permute(1,2,0) permutes value of transfrom 'image' item from tesor to RGB 3 * 224 * 224\n",
        "    refleciton_pad_array = reflect_transforms['training'](image=img)['image'].permute(1,2,0)\n",
        "\n",
        "    print(img.shape)\n",
        "\n",
        "    #zero_pad_array = train_transforms(Image.fromarray(img_array)).permute(1,2,0)\n",
        "    fig, axs = plt.subplots(2,3, figsize=(15,15))\n",
        "    no_transform_fid = read(img_fid)\n",
        "\n",
        "    transforms_dict = {'no_tranform': no_transform_fid,\n",
        "                'from_array': img_array,\n",
        "                'reflection_from_Pad_array': refleciton_pad_array}\n",
        "    for idx_ in range(3):\n",
        "        img_fid = y_g_dict[list(y_g_dict.keys())[idx]]['fid']\n",
        "        img = read(img_fid).astype(np.uint8)\n",
        "        transforms_dict['reflect'+str(idx_)]=reflect_transforms['training'](image=img)['image'].permute(1,2,0)\n",
        "\n",
        "\n",
        "\n",
        "    for item in enumerate(transforms_dict):\n",
        "        idx, key = item\n",
        "        if idx<3:\n",
        "            row=0\n",
        "        else:\n",
        "            row=1\n",
        "            idx-=3\n",
        "        axs[row,idx].imshow(transforms_dict[key])\n",
        "        axs[row,idx].set_title(key)\n",
        "        axs[row,idx].set_ylabel(transforms_dict[key].shape[0])\n",
        "        axs[row,idx].set_xlabel(transforms_dict[key].shape[1])\n",
        "        if str(type(transforms_dict[key]))!=\"<class 'numpy.ndarray'>\":\n",
        "            print(type(transforms_dict[key]) )\n",
        "            cv2.imwrite(key+'.png',transforms_dict[key].numpy().astype(np.uint8))\n",
        "    plt.savefig('transfroms.png', dpi=300)\n",
        "\n",
        "def data_samplers(data, data_class, batch_size=None):\n",
        "    '''retrurns data loaders called during training'''\n",
        "    test_ids = [idx for idx in data['training']][:20]\n",
        "    # a small subset for debugging if needed <^_^> \n",
        "    data_tester = {key:data['training'][key] for key in test_ids}\n",
        "    #change back\n",
        "\n",
        "    train_data_loader =  ava_data_reflect(\n",
        "        data['training'], transform=reflect_transforms['training']\n",
        "        )\n",
        "    val_data_loader =  ava_data_reflect(\n",
        "        data['validation'], transform=reflect_transforms['validation']\n",
        "        )\n",
        "    test_data_loader =  ava_data_reflect(\n",
        "        data['test'], transform=reflect_transforms['test']\n",
        "        )\n",
        "    data_load_dict = {\n",
        "        'training':train_data_loader,\n",
        "        'validation':val_data_loader, \n",
        "        'test': test_data_loader\n",
        "        }\n",
        "    #Let there be 9 samples and 1 sample in class 0 and 1 respectively\n",
        "    labels = [data['training'][idx]['threshold'] for idx in data['training']]\n",
        "    class_counts = np.bincount(labels)\n",
        "    num_samples = sum(class_counts)\n",
        "    #corresponding labels of samples\n",
        "    class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "    weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(\n",
        "        torch.DoubleTensor(weights), int(num_samples)\n",
        "        )\n",
        "    print(len(weights))\n",
        "    print(class_weights)\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(\n",
        "        torch.DoubleTensor(weights), int(len(data['training'].keys()))\n",
        "        )\n",
        "    # with data sampler (note ->> must be same len[-,...,-] as train set!!)\n",
        "    train_loader = DataLoader(\n",
        "        dataset = train_data_loader, \n",
        "        sampler=sampler, \n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "        )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset = val_data_loader, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False\n",
        "        )\n",
        "    test_loader = DataLoader(\n",
        "        dataset = test_data_loader,\n",
        "         batch_size=batch_size, shuffle=True)\n",
        "    return {'training':train_loader,'validation':val_loader, 'test': test_loader}\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def train_model(model, criterion, \n",
        "                optimizer, \n",
        "                scheduler, \n",
        "                num_epochs=None, \n",
        "                model_name = None, \n",
        "                did = None,\n",
        "                data = None):\n",
        "    '''Training and validation loops- 1 loop == one epoch\n",
        "    has a saving fuciton saving model on best epoch\n",
        "    records total train time'''\n",
        "    results = { }\n",
        "    # pathlib path object --> most pythonic option. \n",
        "    did = Path(did)\n",
        "    did = did/model_name\n",
        "    os.makedirs(did,exist_ok=True)\n",
        "    print(f'currently trianing {model_name}')\n",
        "    print(f'{model_name} will be saved at {did/model_name}')\n",
        "    since = time.time()\n",
        "    # copy state dict for best model saving (training could make them worse)\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['training', 'validation']:\n",
        "            dataset_size = len(data[phase])\n",
        "            print(dataset_size)\n",
        "            if phase == 'training':\n",
        "                model.train()   # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            print(phase)\n",
        "            # Iterate over data.\n",
        "            for inputs, labels,fid in tqdm(data_load_dict[phase]):\n",
        "                #wandb.log({\"examples\" : [wandb.Image(im) for im in inputs]})\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'training'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'training':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'training':\n",
        "                scheduler[0].step(scheduler[1])\n",
        "\n",
        "            ballance = np.array([])\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "            class_preds = outputs.argmax(dim=1) \n",
        "            batch_acc = metrics.balanced_accuracy_score(labels.cpu(), \n",
        "                                                         class_preds.cpu())\n",
        "            ballance = np.append(ballance, batch_acc)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            key = 'epoch_'+str(epoch+1)+'_'+phase\n",
        "            results[key]= {\n",
        "                phase+' loss' : epoch_loss, \n",
        "                phase+' acc': float(epoch_acc.cpu()),\n",
        "                phase+ ' ballance_acc':ballance.mean()\n",
        "                }\n",
        "            wandb.log({\n",
        "                phase+' loss' : epoch_loss, \n",
        "                phase+' acc': float(epoch_acc.cpu()),\n",
        "                phase+ ' ballance_acc':ballance.mean()\n",
        "                })\n",
        "            # w mode to overwirte existing json - reading and re writing \n",
        "            # in append modes can cause jsaon formatting issues\n",
        "            # files are json for ease of loading to python dict in evaluation\n",
        "            still_to_train = num_epochs - epoch\n",
        "            save_fid = did/(model_name+f'_epoch_{10-still_to_train}.json')\n",
        "            with open(save_fid, 'w') as handle:\n",
        "                json.dump(results, handle)\n",
        "            print(results)\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'validation' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc \n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save({'epoch':epoch, \n",
        "                            'model_state_dict':model.state_dict(),\n",
        "                            'optimizer_state_dict':optimizer.state_dict()\n",
        "                            }, did/model_name)\n",
        "                wandb.log({'best_epoch':epoch})\n",
        "                print(f'Saving {model_name} in {did.name}')\n",
        "                #model save\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    t_mins, t_seconds = time_elapsed // 60, time_elapsed % 60\n",
        "    train_overall = {\n",
        "        'mins':t_mins, \n",
        "        'seconds':t_seconds, \n",
        "        'best_acc': best_acc.cpu().tolist()\n",
        "        }\n",
        "    wandb.log(train_overall)\n",
        "    print(f'training time = {train_overall}')\n",
        "    with open(did/('train_overall'+'.json'), 'w') as handle:\n",
        "        json.dump(train_overall, handle)\n",
        "\n",
        "\n",
        "def loader(models):\n",
        "    '''genrator for models when loooping through all models'''\n",
        "    for mod in models:\n",
        "        print(mod)\n",
        "        if 'ResNet' in mod:\n",
        "            loaded = torch.load(models[mod])\n",
        "            res_n = models[mod].name.split('_')[-1]\n",
        "            if '18' == res_n:\n",
        "                model = torchvision.models.resnet18(pretrained=True)\n",
        "            elif '50' == res_n:\n",
        "                model = torchvision.models.resnet50(pretrained=True)\n",
        "            else:\n",
        "                model = torchvision.models.resnet152(pretrained=True)\n",
        "            feature_in = model.fc.in_features\n",
        "            model.fc = nn.Linear(feature_in, 2)\n",
        "            model.load_state_dict(loaded['model_state_dict'])\n",
        "        elif'ConViT' in mod:\n",
        "            print(models[mod].parent.name)\n",
        "            loaded = torch.load(models[mod])\n",
        "            print(loaded.keys())\n",
        "            model = timm.create_model(models[mod].name.strip(models[mod].suffix)\n",
        "            , pretrained=True)\n",
        "            model.head = nn.Linear(model.head.in_features,2,bias=True)\n",
        "            if 'pth' in models[mod].name:\n",
        "                model.load_state_dict(loaded['model'])\n",
        "            else:\n",
        "                model.load_state_dict(loaded['model'])\n",
        "        elif 'CvT' in mod:\n",
        "            loaded = torch.load(models[mod])\n",
        "            model = CvT(\n",
        "                        num_classes = 2,\n",
        "                        s1_emb_dim = 64,      \n",
        "                        s1_emb_kernel = 7,     \n",
        "                        s1_emb_stride = 4,      \n",
        "                        s1_proj_kernel = 3,     \n",
        "                        s1_kv_proj_stride = 2,  \n",
        "                        s1_heads = 1,         \n",
        "                        s1_depth = 1,         \n",
        "                        s1_mlp_mult = 4,        \n",
        "                        s2_emb_dim = 192,      \n",
        "                        s2_emb_kernel = 3,\n",
        "                        s2_emb_stride = 2,\n",
        "                        s2_proj_kernel = 3,\n",
        "                        s2_kv_proj_stride = 2,\n",
        "                        s2_heads = 3,\n",
        "                        s2_depth = 2,\n",
        "                        s2_mlp_mult = 4,\n",
        "                        s3_emb_dim = 384,     \n",
        "                        s3_emb_kernel = 3,\n",
        "                        s3_emb_stride = 2,\n",
        "                        s3_proj_kernel = 3,\n",
        "                        s3_kv_proj_stride = 2,\n",
        "                        s3_heads = 4,\n",
        "                        s3_depth = 10,\n",
        "                        s3_mlp_mult = 4,\n",
        "                        dropout = 0.\n",
        "                    )\n",
        "            model.load_state_dict\n",
        "            model.load_state_dict(loaded['model_state_dict'])\n",
        "        elif 'mobilenet' in mod:\n",
        "            print(mod)\n",
        "            model = timm.create_model(mod , pretrained=True)\n",
        "            model.classifier.out_featrues=2\n",
        "        elif 'mobilevit' in mod:\n",
        "            print(mod)\n",
        "            model = timm.create_model(mod , pretrained=True)\n",
        "            model.head.fc.out_features=2\n",
        "        else:\n",
        "            model = timm.create_model(mod , pretrained=True)\n",
        "            model.head.fc.out_features=2\n",
        "        \n",
        "        yield model,models[mod]['epochs'],mod\n",
        "            \n",
        "def deep_eval(model):\n",
        "    '''validatioan loop ruturns metrics dict for passed model'''\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.to(device)\n",
        "    results_dict = { }\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for data, label, fid in tqdm(data_load_dict['test']):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = model(data)            \n",
        "            sm = torch.nn.Softmax(dim=1)\n",
        "            probabilities = sm(output)\n",
        "            for dir_,prob,lab in zip(fid,probabilities,label):\n",
        "                results_dict[dir_.split('/')[-1]]={ \n",
        "                    'class_probs':prob.cpu().tolist(), \n",
        "                    'pred_class': int(prob.argmax(dim=0).cpu()), \n",
        "                    'g_t_class': int(lab.cpu())}\n",
        "            val_loss = criterion(output, label)\n",
        "            acc = (output.argmax(dim=1) == label).float().mean()\n",
        "            results_dict['test_accuracy']={'test_acc':float(acc.cpu())}\n",
        "    return results_dict\n"
      ],
      "metadata": {
        "id": "udf2DpmAU-wy"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ava_data_reflect(Dataset):\n",
        "    '''data class wich is used by data loader retruns transformed image '''\n",
        "    def __init__(self, im_dict, state = None, transform=None):\n",
        "        self.im_dict = im_dict\n",
        "        self.transform = transform\n",
        "        self.files  = list(im_dict.keys())\n",
        "        self.state = state\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.im_dict.keys())\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #img_path = self.im_dict[self.files[idx]]['fid']\n",
        "        # reads symbolic links from test val train dirs returns rgb array\n",
        "        read = lambda fid: cv2.cvtColor(cv2.imread(os.readlink(fid)), cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "        img = self.im_dict[self.files[idx]]['fid']\n",
        "        img = read(img)\n",
        "        # stacks grayscale images \n",
        "        if len(img.shape) !=3:\n",
        "            img = np.stack([np.copy(img) for i in range(3)], axis=2)\n",
        "             \n",
        "        #img = self.a_transform(image=img)['image']\n",
        "        # converst to pillow image from arry\n",
        "        # this is faster as open cv reads image \n",
        "        # faster than pillow\n",
        "        # pillow also returns file read errors\n",
        "        # for some image in ava dataset\n",
        "        # cv2 does not. \n",
        "        #img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
        "        \n",
        "        img_transformed = self.transform(image=img)['image']\n",
        "        # gets one hot (binary) thresholded groud truth\n",
        "\n",
        "        label = int(self.im_dict[self.files[idx]]['threshold'])\n",
        "\n",
        "        # uncomment to check that lable and data loading correctly (debug)\n",
        "        #print(label, self.im_dict[self.files[idx]])\n",
        "\n",
        "        \n",
        "\n",
        "        return img_transformed, label, self.im_dict[self.files[idx]]['fid']"
      ],
      "metadata": {
        "id": "wuE0f3qtVikC"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df,y_g_dict, data, neg, pos = get_all()"
      ],
      "metadata": {
        "id": "hW-JjUdhVpxI",
        "outputId": "befb9120-fe8b-4c8f-ec07-4b174c1d6c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Unnamed: 0  Unnamed: 0.1  1  2   3   4   5   6   7   8  ...  \\\n",
              "0                0             0  0  0   0   5  32  50  23  10  ...   \n",
              "1                1             1  0  3   2   3  13  40  35  21  ...   \n",
              "2                2             2  0  2   3   9  35  50  20   5  ...   \n",
              "3                3             3  0  1   7  26  56  23   6   1  ...   \n",
              "4                4             4  0  1   4   5  33  50  17   9  ...   \n",
              "...            ...           ... .. ..  ..  ..  ..  ..  ..  ..  ...   \n",
              "255497      255497        255497  1  7  17  40  84  56  31  13  ...   \n",
              "255498      255498        255498  1  0   8  25  50  65  55  26  ...   \n",
              "255499      255499        255499  0  0   8  11  26  47  48  56  ...   \n",
              "255500      255500        255500  0  2  19  30  75  60  30  25  ...   \n",
              "255501      255501        255501  1  3  10  39  51  61  39  29  ...   \n",
              "\n",
              "        challenge_id       MOS  image_name  MLS       set  width height  \\\n",
              "0               1396  6.112903  953417.jpg    6  training  781.0  699.0   \n",
              "1               1396  6.593750  953777.jpg    6  training  550.0  800.0   \n",
              "2               1396  5.796875  953756.jpg    6  training  800.0  594.0   \n",
              "3               1396  5.040984  954195.jpg    5  training  587.0  674.0   \n",
              "4               1396  5.943548  953903.jpg    6  training  800.0  533.0   \n",
              "...              ...       ...         ...  ...       ...    ...    ...   \n",
              "255497           181  5.339844   56938.jpg    5  training  640.0  456.0   \n",
              "255498           181  6.235294   57304.jpg    6  training  583.0  640.0   \n",
              "255499           181  7.072581   56711.jpg    8  training  640.0  485.0   \n",
              "255500           181  5.608871   57303.jpg    5  training  640.0  423.0   \n",
              "255501           181  5.853061   57221.jpg    6  training  640.0  484.0   \n",
              "\n",
              "            ID threshold  mos_float  \n",
              "0       953417         1   6.112903  \n",
              "1       953777         1   6.593750  \n",
              "2       953756         1   5.796875  \n",
              "3       954195         1   5.040984  \n",
              "4       953903         1   5.943548  \n",
              "...        ...       ...        ...  \n",
              "255497   56938         1   5.339844  \n",
              "255498   57304         1   6.235294  \n",
              "255499   56711         1   7.072581  \n",
              "255500   57303         1   5.608871  \n",
              "255501   57221         1   5.853061  \n",
              "\n",
              "[255502 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2fdd1da8-70f4-4235-9068-43dc4726f37f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>challenge_id</th>\n",
              "      <th>MOS</th>\n",
              "      <th>image_name</th>\n",
              "      <th>MLS</th>\n",
              "      <th>set</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>ID</th>\n",
              "      <th>threshold</th>\n",
              "      <th>mos_float</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>50</td>\n",
              "      <td>23</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>1396</td>\n",
              "      <td>6.112903</td>\n",
              "      <td>953417.jpg</td>\n",
              "      <td>6</td>\n",
              "      <td>training</td>\n",
              "      <td>781.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>953417</td>\n",
              "      <td>1</td>\n",
              "      <td>6.112903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>21</td>\n",
              "      <td>...</td>\n",
              "      <td>1396</td>\n",
              "      <td>6.593750</td>\n",
              "      <td>953777.jpg</td>\n",
              "      <td>6</td>\n",
              "      <td>training</td>\n",
              "      <td>550.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>953777</td>\n",
              "      <td>1</td>\n",
              "      <td>6.593750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>35</td>\n",
              "      <td>50</td>\n",
              "      <td>20</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>1396</td>\n",
              "      <td>5.796875</td>\n",
              "      <td>953756.jpg</td>\n",
              "      <td>6</td>\n",
              "      <td>training</td>\n",
              "      <td>800.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>953756</td>\n",
              "      <td>1</td>\n",
              "      <td>5.796875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>26</td>\n",
              "      <td>56</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1396</td>\n",
              "      <td>5.040984</td>\n",
              "      <td>954195.jpg</td>\n",
              "      <td>5</td>\n",
              "      <td>training</td>\n",
              "      <td>587.0</td>\n",
              "      <td>674.0</td>\n",
              "      <td>954195</td>\n",
              "      <td>1</td>\n",
              "      <td>5.040984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>33</td>\n",
              "      <td>50</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>...</td>\n",
              "      <td>1396</td>\n",
              "      <td>5.943548</td>\n",
              "      <td>953903.jpg</td>\n",
              "      <td>6</td>\n",
              "      <td>training</td>\n",
              "      <td>800.0</td>\n",
              "      <td>533.0</td>\n",
              "      <td>953903</td>\n",
              "      <td>1</td>\n",
              "      <td>5.943548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255497</th>\n",
              "      <td>255497</td>\n",
              "      <td>255497</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>17</td>\n",
              "      <td>40</td>\n",
              "      <td>84</td>\n",
              "      <td>56</td>\n",
              "      <td>31</td>\n",
              "      <td>13</td>\n",
              "      <td>...</td>\n",
              "      <td>181</td>\n",
              "      <td>5.339844</td>\n",
              "      <td>56938.jpg</td>\n",
              "      <td>5</td>\n",
              "      <td>training</td>\n",
              "      <td>640.0</td>\n",
              "      <td>456.0</td>\n",
              "      <td>56938</td>\n",
              "      <td>1</td>\n",
              "      <td>5.339844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255498</th>\n",
              "      <td>255498</td>\n",
              "      <td>255498</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>25</td>\n",
              "      <td>50</td>\n",
              "      <td>65</td>\n",
              "      <td>55</td>\n",
              "      <td>26</td>\n",
              "      <td>...</td>\n",
              "      <td>181</td>\n",
              "      <td>6.235294</td>\n",
              "      <td>57304.jpg</td>\n",
              "      <td>6</td>\n",
              "      <td>training</td>\n",
              "      <td>583.0</td>\n",
              "      <td>640.0</td>\n",
              "      <td>57304</td>\n",
              "      <td>1</td>\n",
              "      <td>6.235294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255499</th>\n",
              "      <td>255499</td>\n",
              "      <td>255499</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>26</td>\n",
              "      <td>47</td>\n",
              "      <td>48</td>\n",
              "      <td>56</td>\n",
              "      <td>...</td>\n",
              "      <td>181</td>\n",
              "      <td>7.072581</td>\n",
              "      <td>56711.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>training</td>\n",
              "      <td>640.0</td>\n",
              "      <td>485.0</td>\n",
              "      <td>56711</td>\n",
              "      <td>1</td>\n",
              "      <td>7.072581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255500</th>\n",
              "      <td>255500</td>\n",
              "      <td>255500</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>30</td>\n",
              "      <td>75</td>\n",
              "      <td>60</td>\n",
              "      <td>30</td>\n",
              "      <td>25</td>\n",
              "      <td>...</td>\n",
              "      <td>181</td>\n",
              "      <td>5.608871</td>\n",
              "      <td>57303.jpg</td>\n",
              "      <td>5</td>\n",
              "      <td>training</td>\n",
              "      <td>640.0</td>\n",
              "      <td>423.0</td>\n",
              "      <td>57303</td>\n",
              "      <td>1</td>\n",
              "      <td>5.608871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255501</th>\n",
              "      <td>255501</td>\n",
              "      <td>255501</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>39</td>\n",
              "      <td>51</td>\n",
              "      <td>61</td>\n",
              "      <td>39</td>\n",
              "      <td>29</td>\n",
              "      <td>...</td>\n",
              "      <td>181</td>\n",
              "      <td>5.853061</td>\n",
              "      <td>57221.jpg</td>\n",
              "      <td>6</td>\n",
              "      <td>training</td>\n",
              "      <td>640.0</td>\n",
              "      <td>484.0</td>\n",
              "      <td>57221</td>\n",
              "      <td>1</td>\n",
              "      <td>5.853061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>255502 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fdd1da8-70f4-4235-9068-43dc4726f37f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2fdd1da8-70f4-4235-9068-43dc4726f37f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2fdd1da8-70f4-4235-9068-43dc4726f37f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255502\n",
            "255411\n",
            "255403 255403\n",
            "2 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.7220, 0.7046])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[38;2;255;105;180m██████████\u001b[0m| 19899/19899 [00:00<00:00, 60139.60it/s]\n",
            "100%|\u001b[38;2;255;105;180m██████████\u001b[0m| 235121/235121 [00:04<00:00, 57072.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set n = 223363 \n",
            "test_list n = 19899\n",
            "validation_list n = 11758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/mobilevit_xxs/mobilevit_xxs"
      ],
      "metadata": {
        "id": "VIiQ2P4ALx89",
        "outputId": "90d05c24-17bb-473f-d360-256d93c0c221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/mobilevit_xxs/mobilevit_xxs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = timm.create_model('mobilevit_xxs')"
      ],
      "metadata": {
        "id": "c-w8KeifN6bS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.head.fc.out_features=2"
      ],
      "metadata": {
        "id": "uGF6Kr7pOKQx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded = torch.load('drive/MyDrive/mobilevit_xxs/mobilevit_xxs')"
      ],
      "metadata": {
        "id": "79L4zLNCOkFP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(loaded['model_state_dict'])"
      ],
      "metadata": {
        "id": "21KeWiQ2PXN_",
        "outputId": "3d05d140-be51-4b4b-9080-b9c0d36e8137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_test_transform = A.Compose([\n",
        "            A.augmentations.geometric.resize.LongestMaxSize(max_size=224),\n",
        "            A.augmentations.transforms.PadIfNeeded(224,224),\n",
        "            A.augmentations.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), \n",
        "                                                max_pixel_value=255.0, p=1.0),\n",
        "                            A.pytorch.transforms.ToTensorV2(transpose_mask=False, p=1.0) ]\n",
        "                            )"
      ],
      "metadata": {
        "id": "wbDyprW4QF9C"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_attention_map(x, get_mask=False):\n",
        "    #x.size()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, att_mat = model(x.unsqueeze(0))\n",
        "\n",
        "    att_mat = torch.stack(att_mat).squeeze(1)\n",
        "\n",
        "    # Average the attention weights across all heads.\n",
        "    att_mat = torch.mean(att_mat, dim=1)\n",
        "\n",
        "    # To account for residual connections, we add an identity matrix to the\n",
        "    # attention matrix and re-normalize the weights.\n",
        "    residual_att = torch.eye(att_mat.size(1))\n",
        "    aug_att_mat = att_mat + residual_att\n",
        "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
        "\n",
        "    # Recursively multiply the weight matrices\n",
        "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
        "    joint_attentions[0] = aug_att_mat[0]\n",
        "\n",
        "    for n in range(1, aug_att_mat.size(0)):\n",
        "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
        "\n",
        "    v = joint_attentions[-1]\n",
        "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
        "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
        "    if get_mask:\n",
        "        result = cv2.resize(mask / mask.max(), img.size)\n",
        "    else:        \n",
        "        mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n",
        "        result = (mask * img).astype(\"uint8\")\n",
        "    \n",
        "    return result"
      ],
      "metadata": {
        "id": "-NYfOS2qJFNW"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reflect_transforms = data_transforms(size=224)\n",
        "\n",
        "test_data_loader =  ava_data_reflect(\n",
        "        data['test'], transform=reflect_transforms['test']\n",
        "        )\n",
        "test_loader = DataLoader(\n",
        "        dataset = test_data_loader,\n",
        "         batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "8oMk1FbSPluQ",
        "outputId": "0882ab1e-3044-4d81-f8c0-3a28c623d49f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-50ac3a51ef8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m test_data_loader =  ava_data_reflect(\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreflect_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         )\n\u001b[1;32m      6\u001b[0m test_loader = DataLoader(\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ldr = iter(test_data_loader)"
      ],
      "metadata": {
        "id": "WRznW4KUZMfU"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, lable, dir_ = next(ldr)\n",
        "model = model.eval()\n",
        "with torch.no_grad():\n",
        "    model(data.unsqueeze(0))"
      ],
      "metadata": {
        "id": "z-amvzDoZY_Q"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.stages."
      ],
      "metadata": {
        "id": "LRzo3rG1PxGr",
        "outputId": "b345b8b4-9c15-418d-ed94-7ba0fceacdae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (shortcut): Identity()\n",
              "      (conv1_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2b_kxk): Identity()\n",
              "      (attn): Identity()\n",
              "      (conv3_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "      )\n",
              "      (attn_last): Identity()\n",
              "      (drop_path): Identity()\n",
              "      (act): Identity()\n",
              "    )\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (conv1_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2b_kxk): Identity()\n",
              "      (attn): Identity()\n",
              "      (conv3_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "      )\n",
              "      (attn_last): Identity()\n",
              "      (drop_path): Identity()\n",
              "      (act): Identity()\n",
              "    )\n",
              "    (1): BottleneckBlock(\n",
              "      (shortcut): Identity()\n",
              "      (conv1_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2b_kxk): Identity()\n",
              "      (attn): Identity()\n",
              "      (conv3_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "      )\n",
              "      (attn_last): Identity()\n",
              "      (drop_path): Identity()\n",
              "      (act): Identity()\n",
              "    )\n",
              "    (2): BottleneckBlock(\n",
              "      (shortcut): Identity()\n",
              "      (conv1_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2b_kxk): Identity()\n",
              "      (attn): Identity()\n",
              "      (conv3_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "      )\n",
              "      (attn_last): Identity()\n",
              "      (drop_path): Identity()\n",
              "      (act): Identity()\n",
              "    )\n",
              "  )\n",
              "  (2): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (conv1_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2b_kxk): Identity()\n",
              "      (attn): Identity()\n",
              "      (conv3_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "      )\n",
              "      (attn_last): Identity()\n",
              "      (drop_path): Identity()\n",
              "      (act): Identity()\n",
              "    )\n",
              "    (1): MobileViTBlock(\n",
              "      (conv_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv_1x1): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (transformer): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (conv_proj): ConvNormAct(\n",
              "        (conv): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv_fusion): ConvNormAct(\n",
              "        (conv): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (3): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (conv1_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2b_kxk): Identity()\n",
              "      (attn): Identity()\n",
              "      (conv3_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "      )\n",
              "      (attn_last): Identity()\n",
              "      (drop_path): Identity()\n",
              "      (act): Identity()\n",
              "    )\n",
              "    (1): MobileViTBlock(\n",
              "      (conv_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv_1x1): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (transformer): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=80, out_features=80, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=80, out_features=80, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): Block(\n",
              "          (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=80, out_features=80, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (3): Block(\n",
              "          (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=80, out_features=80, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
              "      (conv_proj): ConvNormAct(\n",
              "        (conv): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv_fusion): ConvNormAct(\n",
              "        (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (4): Sequential(\n",
              "    (0): BottleneckBlock(\n",
              "      (conv1_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv2b_kxk): Identity()\n",
              "      (attn): Identity()\n",
              "      (conv3_1x1): ConvNormAct(\n",
              "        (conv): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "      )\n",
              "      (attn_last): Identity()\n",
              "      (drop_path): Identity()\n",
              "      (act): Identity()\n",
              "    )\n",
              "    (1): MobileViTBlock(\n",
              "      (conv_kxk): ConvNormAct(\n",
              "        (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv_1x1): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (transformer): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): Block(\n",
              "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
              "            (act): SiLU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "      (conv_proj): ConvNormAct(\n",
              "        (conv): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (conv_fusion): ConvNormAct(\n",
              "        (conv): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNormAct2d(\n",
              "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([1])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "9FShQ23pQNBM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gn2hk-vVTdRW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "visualize_attention.ipynb",
      "provenance": [],
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}